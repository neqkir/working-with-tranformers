{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOseFWpRiiVjpSaqjEUY7kw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neqkir/working-with-tranformers/blob/main/BERT/bertsumabs-text-summarization\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "QUICK_RUN = True"
      ],
      "metadata": {
        "id": "gqJpyscX9IqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wDIqBilErXYC",
        "outputId": "65b9a6a4-547a-4ce1-a70d-c9be856a32d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: eval in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (3.10.0.2)\n",
            "Requirement already satisfied: pyrouge in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: scrapbook in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: papermill in /usr/local/lib/python3.7/dist-packages (from scrapbook) (2.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scrapbook) (1.1.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from scrapbook) (3.0.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from scrapbook) (5.5.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from scrapbook) (2.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (7.1.2)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (2.23.0)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (21.12b0)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (4.62.3)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.5.9)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (8.0.1)\n",
            "Requirement already satisfied: ansiwrap in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.8.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.5.4)\n",
            "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (7.1.0)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (4.9.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (22.3.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (0.2.0)\n",
            "Requirement already satisfied: textwrap3>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from ansiwrap->papermill->scrapbook) (0.9.2)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (3.10.0.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (1.5.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (0.4.3)\n",
            "Requirement already satisfied: pathspec<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (0.9.0)\n",
            "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (1.2.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->scrapbook) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2021.10.8)\n",
            "Requirement already satisfied: indicnlp in /usr/local/lib/python3.7/dist-packages (0.0.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bb728b3b58f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummarizationDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_nlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnndm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNNDMSummarizationDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/utils_nlp/eval/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_rouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_rouge_perl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_rouge_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/utils_nlp/eval/rouge/compute_rouge.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyrouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouge155\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrouge_ext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRougeExt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/utils_nlp/eval/rouge/rouge_ext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindicnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindic_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mlanguage_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhindi_stemmer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhi_stem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'indicnlp.tokenize'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install --upgrade \n",
        "!pip install -q git+https://github.com/microsoft/nlp-recipes.git\n",
        "!pip install transformers\n",
        "!pip install eval\n",
        "!pip install rouge\n",
        "!pip install jsonlines\n",
        "!pip install pyrouge\n",
        "!pip install scrapbook\n",
        "!pip install indicnlp\n",
        "#!pip install indicnlp.tokenize\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from tempfile import TemporaryDirectory\n",
        "import torch\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import scrapbook as sb\n",
        "\n",
        "nlp_path = os.path.abspath(\"../../\")\n",
        "if nlp_path not in sys.path:\n",
        "    sys.path.insert(0, nlp_path)\n",
        "\n",
        "from utils_nlp import models\n",
        "from utils_nlp.models import transformers \n",
        "from utils_nlp.models.transformers.datasets import SummarizationDataset\n",
        "from utils_nlp import eval\n",
        "from utils_nlp.eval import rouge\n",
        "from utils_nlp.dataset.cnndm import CNNDMSummarizationDataset\n",
        "from utils_nlp.eval import compute_rouge_python\n",
        "\n",
        "from utils_nlp.models.transformers.abstractive_summarization_bertsum \\\n",
        "     import BertSumAbs, BertSumAbsProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation examples and ~11K test examples. The length of the news articles is 781 tokens on average and the summaries are of 3.75 sentences and 56 tokens on average."
      ],
      "metadata": {
        "id": "1leWQsmC9oxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the data path used to save the downloaded data file\n",
        "DATA_PATH = TemporaryDirectory().name\n",
        "# The number of lines at the head of data file used for preprocessing. -1 means all the lines.\n",
        "TOP_N = 100\n",
        "if not QUICK_RUN:\n",
        "    TOP_N = -1\n",
        "\n",
        "train_dataset, test_dataset = CNNDMSummarizationDataset(\n",
        "    top_n=TOP_N, local_cache_path=DATA_PATH, prepare_extractive=False\n",
        ")"
      ],
      "metadata": {
        "id": "k6DDOzdB5Jbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model finetuning"
      ],
      "metadata": {
        "id": "KVKLCN4x5001"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# notebook parameters\n",
        "# the cache path\n",
        "CACHE_PATH = TemporaryDirectory().name\n",
        "\n",
        "# model parameters\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "MAX_POS = 768\n",
        "MAX_SOURCE_SEQ_LENGTH = 640\n",
        "MAX_TARGET_SEQ_LENGTH = 140\n",
        "\n",
        "# mixed precision setting. To enable mixed precision training, follow instructions in SETUP.md.\n",
        "FP16 = False\n",
        "if FP16:\n",
        "    FP16_OPT_LEVEL = \"O2\"\n",
        "\n",
        "# fine-tuning parameters\n",
        "# batch size, unit is the number of tokens\n",
        "BATCH_SIZE_PER_GPU = 1\n",
        "\n",
        "\n",
        "# GPU used for training\n",
        "NUM_GPUS = torch.cuda.device_count()\n",
        "if NUM_GPUS > 0:\n",
        "    BATCH_SIZE = NUM_GPUS * BATCH_SIZE_PER_GPU\n",
        "else:\n",
        "    BATCH_SIZE = 1\n",
        "\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE_BERT = 5e-4 / 2.0\n",
        "LEARNING_RATE_DEC = 0.05 / 2.0\n",
        "\n",
        "\n",
        "# How often the statistics reports show up in training, unit is step.\n",
        "REPORT_EVERY = 10\n",
        "SAVE_EVERY = 500\n",
        "\n",
        "# total number of steps for training\n",
        "MAX_STEPS = 1e3\n",
        "\n",
        "if not QUICK_RUN:\n",
        "    MAX_STEPS = 5e3\n",
        "\n",
        "WARMUP_STEPS_BERT = 2000\n",
        "WARMUP_STEPS_DEC = 1000"
      ],
      "metadata": {
        "id": "2lAg5-Mf54vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processor which contains the colloate function to load the preprocessed data\n",
        "processor = BertSumAbsProcessor(cache_dir=CACHE_PATH, max_src_len=MAX_SOURCE_SEQ_LENGTH, max_tgt_len=MAX_TARGET_SEQ_LENGTH)\n",
        "# summarizer\n",
        "summarizer = BertSumAbs(\n",
        "    processor, cache_dir=CACHE_PATH, max_pos_length=MAX_POS\n",
        ")"
      ],
      "metadata": {
        "id": "rDJTu9WG6g7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE_PER_GPU*NUM_GPUS"
      ],
      "metadata": {
        "id": "afT_48Zn6kFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.fit(\n",
        "    train_dataset,\n",
        "    num_gpus=NUM_GPUS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_steps=MAX_STEPS,\n",
        "    learning_rate_bert=LEARNING_RATE_BERT,\n",
        "    learning_rate_dec=LEARNING_RATE_DEC,\n",
        "    warmup_steps_bert=WARMUP_STEPS_BERT,\n",
        "    warmup_steps_dec=WARMUP_STEPS_DEC,\n",
        "    save_every=SAVE_EVERY,\n",
        "    report_every=REPORT_EVERY * 5,\n",
        "    fp16=FP16,\n",
        "    # checkpoint=\"saved checkpoint path\"\n",
        ")"
      ],
      "metadata": {
        "id": "oEG6Gh0e6mqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.save_model(MAX_STEPS, os.path.join(CACHE_PATH, \"bertsumabs.pt\"))"
      ],
      "metadata": {
        "id": "xIYMnxvG88GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation\n",
        "\n",
        "To run rouge evaluation, please refer to the section of compute_rouge_perl in summarization_evaluation.ipynb for setup. For the settings in this notebook with QUICK_RUN=False, you should get ROUGE scores close to the following numbers:\n",
        "```\n",
        "{'rouge-1': {'f': 0.34819639878321873, 'p': 0.39977932634737307, \n",
        "'r': 0.34429079596863604}, \n",
        "'rouge-2': {'f': 0.13919271352557894, 'p': 0.16129965067780644, \n",
        "'r': 0.1372938054050938}, \n",
        "'rouge-l': {'f': 0.2313282318854973, 'p': 0.26664667422849747, \n",
        "'r': 0.22850294283399628}}\n",
        "```\n",
        "Better performance can be achieved by increasing the MAX_STEPS."
      ],
      "metadata": {
        "id": "tVhZkrXF9F6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_TOP_N = 32\n",
        "if not QUICK_RUN:\n",
        "    TEST_TOP_N = len(test_dataset)\n",
        "\n",
        "if NUM_GPUS:\n",
        "    BATCH_SIZE = NUM_GPUS * BATCH_SIZE_PER_GPU\n",
        "else:\n",
        "    BATCH_SIZE = 1\n",
        "    \n",
        "shortened_dataset = test_dataset.shorten(top_n=TEST_TOP_N)\n",
        "src = shortened_dataset.get_source()\n",
        "reference_summaries = [\" \".join(t).rstrip(\"\\n\") for t in shortened_dataset.get_target()]\n",
        "generated_summaries = summarizer.predict(\n",
        "    shortened_dataset, batch_size=BATCH_SIZE, num_gpus=NUM_GPUS\n",
        ")\n",
        "assert len(generated_summaries) == len(reference_summaries)"
      ],
      "metadata": {
        "id": "ZGpkz4vW9cbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src[0]"
      ],
      "metadata": {
        "id": "V998G9xW9iwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries[0]"
      ],
      "metadata": {
        "id": "0WL-w9kw9k_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_summaries[0]"
      ],
      "metadata": {
        "id": "_SYwcpcT9mPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_scores = compute_rouge_python(cand=generated_summaries, ref=reference_summaries)\n",
        "pprint.pprint(rouge_scores)"
      ],
      "metadata": {
        "id": "0arQTZO-9oBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing\n",
        "sb.glue(\"rouge_2_f_score\", rouge_scores['rouge-2']['f'])"
      ],
      "metadata": {
        "id": "GrtuPya29p6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "E73glRQ99tDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = \"\"\"\n",
        "But under the new rule, set to be announced in the next 48 hours, Border Patrol agents would immediately return anyone to Mexico — without any detainment and without any due process — who attempts to cross the southwestern border between the legal ports of entry. The person would not be held for any length of time in an American facility.\n",
        "\n",
        "Although they advised that details could change before the announcement, administration officials said the measure was needed to avert what they fear could be a systemwide outbreak of the coronavirus inside detention facilities along the border. Such an outbreak could spread quickly through the immigrant population and could infect large numbers of Border Patrol agents, leaving the southwestern border defenses weakened, the officials argued.\n",
        "The Trump administration plans to immediately turn back all asylum seekers and other foreigners attempting to enter the United States from Mexico illegally, saying the nation cannot risk allowing the coronavirus to spread through detention facilities and Border Patrol agents, four administration officials said.\n",
        "The administration officials said the ports of entry would remain open to American citizens, green-card holders and foreigners with proper documentation. Some foreigners would be blocked, including Europeans currently subject to earlier travel restrictions imposed by the administration. The points of entry will also be open to commercial traffic.\"\"\""
      ],
      "metadata": {
        "id": "5CbAflwJ9uIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SummarizationDataset(\n",
        "    None, source=[source], source_preprocessing=[tokenize.sent_tokenize],\n",
        ")\n",
        "generated_summaries = summarizer.predict(test_dataset, batch_size=1, num_gpus=NUM_GPUS)"
      ],
      "metadata": {
        "id": "1iq7n7d89v_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries[0]"
      ],
      "metadata": {
        "id": "vFOUg02K9xJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(DATA_PATH):\n",
        "    shutil.rmtree(DATA_PATH, ignore_errors=True)\n",
        "if os.path.exists(CACHE_PATH):\n",
        "    shutil.rmtree(CACHE_PATH, ignore_errors=True)"
      ],
      "metadata": {
        "id": "g9jzOWe690cD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
